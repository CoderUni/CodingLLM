seed: 42
model:
  lora:
    r: 32
    alpha: 64
    dropout: 0
  temperature: 0.6
  top_p: 0.95
  top_k: 20
  max_seq_length: 32000

training:
  batch_size: 1
  gradient_accumulation: 16
  learning_rate: 2e-5 # 2e-5 for stage 1-3, 1e-5 for stage 4
  weight_decay: 0.01 # 0.01 for stage 1-3, 0.001 for stage 4
  warmup_ratio: 0.03
  max_steps: 1000
  early_stopping:
    patience: 4
    threshold: 0.002
  test_ratio: 0.2  # For stage 1 and 2 I used 0.2, stage 3 I used 0.1, stage 4 I used 0.05

eval:
  batch_size: 1
  gradient_accumulation: 16
  eval_steps: 100
  metrics:
    patience: 4
    threshold: 0.002

save:
  steps: 100
  limit: 20
  output_dir: "/mnt/storage/metnet/coding_llm/stage4_final_checkpoints"
  # Repo id for pushing model to Hugging Face and ModelScope
  repo_id: "user/coding-model"

logging:
  logging_steps: 10
  wandb_project: "codingLLM" # name of your W&B project
  wandb_log_model: "checkpoint" # log all model checkpoints
  run_name: "qwen-final"
