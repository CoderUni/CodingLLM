{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdcs9-fWU28S"
      },
      "source": [
        "### 1. Detect CUDA version and install the pre-built binaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlJanf9KJXSo",
        "outputId": "7dd356d0-d84d-4440-f412-db1569e5dd0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU Detected:\n",
            " | NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl (551.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m311.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m216.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m177.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m178.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m213.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.5\n",
            "    Uninstalling numpy-2.3.5:\n",
            "      Successfully uninstalled numpy-2.3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.3.16\n",
            "    Uninstalling llama_cpp_python-0.3.16:\n",
            "      Successfully uninstalled llama_cpp_python-0.3.16\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.16 numpy-2.3.5 typing-extensions-4.15.0\n",
            "‚úÖ Installation Complete.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Check for GPU\n",
        "try:\n",
        "    gpu_info = subprocess.check_output([\"nvidia-smi\"]).decode(\"utf-8\")\n",
        "    print(\"GPU Detected:\\n\", gpu_info.split('\\n')[2])\n",
        "except:\n",
        "    raise ValueError(\"‚ùå NO GPU DETECTED! Go to Runtime > Change runtime type > T4 GPU\")\n",
        "\n",
        "# Install the library. Try cu121 if cu124 doesn't work\n",
        "!python -m pip install llama-cpp-python \\\n",
        "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124 \\\n",
        "  --upgrade --force-reinstall --no-cache-dir\n",
        "\n",
        "print(\"Installation Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sRRVF6rU-n1"
      },
      "source": [
        "## 2. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxSrrqrUTZ8S"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import ctypes\n",
        "import torch\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeoHLubGVFix"
      },
      "source": [
        "### Free GPU memory method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_uGec1OJXSq"
      },
      "outputs": [],
      "source": [
        "def free_memory(model_instance):\n",
        "    # Close the model connection\n",
        "    if hasattr(model_instance, 'close'):\n",
        "        model_instance.close()\n",
        "        print(\"Model connection closed.\")\n",
        "\n",
        "    # Delete the Python object to release references\n",
        "    del model_instance\n",
        "    print(\"Python object deleted.\")\n",
        "\n",
        "    # Force Python's Garbage Collector to clear the object\n",
        "    gc.collect()\n",
        "\n",
        "    # Force the C++ backend to release memory to the OS\n",
        "    # This specifically trims the heap memory that llama.cpp allocates\n",
        "    try:\n",
        "        libc = ctypes.CDLL(\"libc.so.6\")\n",
        "        libc.malloc_trim(0)\n",
        "        print(\"C++ memory freed\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not free C++ memory: {e}\")\n",
        "\n",
        "    print(\"\\n GPU Memory should now be freed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V74tS8c0JXSp"
      },
      "source": [
        "### ÊúÄÂ•ΩÁî®Ëã±ÊñáÈ¢òÔºÅ\n",
        "\n",
        "# üöÄ Local Inference on GPU (GPU Êú¨Âú∞Êé®ÁêÜ)\n",
        "\n",
        "This notebook runs the **BigJuicyData/Anni-Q4_K_M-GGUF** model locally on your specific GPU instance. (T4 GPU in Google Colab)\n",
        "\n",
        "</br>\n",
        "\n",
        "# **üöÄ GPU Êú¨Âú∞Êé®ÁêÜ**\n",
        "\n",
        "Êú¨ Notebook Â∞áÂú®ÊÇ®ÁâπÂÆöÁöÑ GPU ÂØ¶‰æãÔºàGoogle Colab ‰∏≠ÁöÑ **T4 GPU**Ôºâ‰∏äÊú¨Âú∞ÈÅãË°å **BigJuicyData/Anni-Q4_K_M-GGUF** Ê®°Âûã„ÄÇ\n",
        "\n",
        "</br>\n",
        "\n",
        "---\n",
        "</br>\n",
        "\n",
        "### How to Use (Â¶Ç‰Ωï‰ΩøÁî®)\n",
        "1. **Edit the Prompt:** Scroll down to the code cell and find the variable `PROMPT = \"...\"`. Replace the text inside with your specific coding question.\n",
        "2. **Run the Cell:** Press the Play button (‚ñ∂Ô∏è) or `Shift + Enter`.\n",
        "3. **Wait for Stream:** The model will \"think\" first (showing logic), followed by the code.\n",
        "\n",
        "\n",
        "### Â¶Ç‰Ωï‰ΩøÁî®\n",
        "1. **Á∑®ËºØÊèêÁ§∫Ë©û (Prompt)Ôºö** Âêë‰∏ãÊªæÂãïÂà∞Á®ãÂºèÁ¢ºÂñÆÂÖÉÊ†ºÔºåÊâæÂà∞ËÆäÊï∏ `PROMPT = \"...\"`„ÄÇÂ∞áÂÖ∂‰∏≠ÁöÑÊñáÂ≠óÊõøÊèõÁÇ∫ÊÇ®ÂÖ∑È´îÁöÑÁ®ãÂºèË®≠Ë®àÂïèÈ°å„ÄÇ\n",
        "2. **ÈÅãË°åÂñÆÂÖÉÊ†ºÔºö** Êåâ‰∏ãÊí≠ÊîæÊåâÈàï (‚ñ∂Ô∏è) Êàñ `Shift + Enter` Èçµ„ÄÇ\n",
        "3. **Á≠âÂæÖÊµÅÂºèËº∏Âá∫Ôºö** Ê®°ÂûãÂ∞áÂÖàÈÄ≤Ë°å„ÄåÊÄùËÄÉ„ÄçÔºàÈ°ØÁ§∫ÈÇèËºØÔºâÔºåÈö®ÂæåËº∏Âá∫Á®ãÂºèÁ¢º„ÄÇ\n",
        "\n",
        "</br>\n",
        "\n",
        "---\n",
        "\n",
        "</br>\n",
        "\n",
        "### Where is the output? (Ëº∏Âá∫Âú®Âì™Ë£°?)\n",
        "The output streams at the **very bottom** of the cell output area. Look for a block formatted like this:\n",
        "\n",
        "```python\n",
        "# The output will look like this and is wrapped inside\n",
        "# ```python\n",
        "def solution():\n",
        "    pass\n",
        "# ```\n",
        "```\n",
        "\n",
        "### Ëº∏Âá∫Âú®Âì™Ë£°Ôºü\n",
        "Ëº∏Âá∫Â∞áÂú®ÂñÆÂÖÉÊ†ºËº∏Âá∫ÂçÄÂüüÁöÑÊúÄÂ∫ïÈÉ®‰ª•ÊµÅÂºèÂÇ≥Ëº∏„ÄÇË´ãÂ∞ãÊâæÂ¶Ç‰∏ãÊ†ºÂºèÁöÑÁ®ãÂºèÁ¢ºÂ°äÔºö\n",
        "\n",
        "```python\n",
        "# The output will look like this and is wrapped inside\n",
        "# ```python\n",
        "def solution():\n",
        "    pass\n",
        "# ```\n",
        "```\n",
        "\n",
        "</br>\n",
        "\n",
        "---\n",
        "\n",
        "</br>\n",
        "\n",
        "#### **IF YOU ENCOUNTER AN ERROR, do this!** Go to Runtime > Restart session and run all (Â¶ÇÊûúÊÇ®ÈÅáÂà∞ÈåØË™§ÔºåË´ãÈÄôÊ®£ÂÅöÔºÅ)\n",
        "- Restart the session to clear the GPU RAM!\n",
        "\n",
        "</br>\n",
        "\n",
        "#### **Â¶ÇÊûúÊÇ®ÈÅáÂà∞ÈåØË™§ÔºåË´ãÈÄôÊ®£ÂÅöÔºÅ** ÂâçÂæÄ Runtime (ÈÅãË°åÊôÇ) > Restart session (ÈáçÊñ∞ÂïüÂãïÊúÉË©±) and run all (‰∏¶ÈÅãË°åÂÖ®ÈÉ®ÂñÆÂÖÉÊ†º)\n",
        "\n",
        "- ÈáçÊñ∞ÂïüÂãïÊúÉË©±‰ª•Ê∏ÖÈô§ GPU Ë®òÊÜ∂È´î (RAM)ÔºÅ\n",
        "\n",
        "</br>\n",
        "---\n",
        "\n",
        "Model page (Ê®°ÂûãÈ†ÅÈù¢):\n",
        "1. [GGUF](https://huggingface.co/BigJuicyData/coder-final-Q4_K_M-GGUF)\n",
        "2. **VLLM**: [Huggingface](https://huggingface.co/BigJuicyData/coder-final) , [Modelscope](https://modelscope.cn/models/quanteat/coder-final)\n",
        "3. [MLX](https://huggingface.co/BigJuicyData/coder-final-mlx-4Bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wxpMsNhJXSq",
        "outputId": "e26d727c-0fd4-4dac-af0c-299084fbec61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14980 MiB free\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 443 tensors from /root/.cache/huggingface/hub/models--BigJuicyData--coder-final-Q4_K_M-GGUF/snapshots/ee6cc085a771a1641d29006cb7f721cc1f70c98d/./coder-final-q4_k_m.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Coder Final\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
            "llama_model_loader: - kv   4:                          qwen3.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                       qwen3.context_length u32              = 40960\n",
            "llama_model_loader: - kv   6:                     qwen3.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  qwen3.feed_forward_length u32              = 17408\n",
            "llama_model_loader: - kv   8:                 qwen3.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:              qwen3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  12:                 qwen3.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               qwen3.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  24:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:  161 tensors\n",
            "llama_model_loader: - type q4_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 8.38 GiB (4.87 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load:   - 151662 ('<|fim_pad|>')\n",
            "load:   - 151663 ('<|repo_name|>')\n",
            "load:   - 151664 ('<|file_sep|>')\n",
            "load: special tokens cache size = 26\n",
            "load: token to piece cache size = 0.9311 MB\n",
            "print_info: arch             = qwen3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 40960\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 5\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 17408\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 40960\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 14B\n",
            "print_info: model params     = 14.77 B\n",
            "print_info: general.name     = Coder Final\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 11 ','\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151654 '<|vision_pad|>'\n",
            "print_info: LF token         = 198 'ƒä'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  36 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  37 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  38 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  39 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  40 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 40 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 41/41 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  8161.75 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   417.30 MiB\n",
            "..........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 24000\n",
            "llama_context: n_ctx_per_seq = 24000\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (24000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 24000 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  32: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  33: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  34: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  35: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  36: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  37: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  38: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  39: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  3750.00 MiB\n",
            "llama_kv_cache_unified: size = 3750.00 MiB ( 24000 cells,  40 layers,  1/1 seqs), K (f16): 1875.00 MiB, V (f16): 1875.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 3544\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:      CUDA0 compute buffer size =  1965.88 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    60.88 MiB\n",
            "llama_context: graph nodes  = 1566\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151654', 'tokenizer.ggml.eos_token_id': '151645', 'qwen3.attention.value_length': '128', 'general.architecture': 'qwen3', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = \\'\\' %}\\n    {%- endif %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is string %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in content %}\\n                {%- set reasoning_content = content.split(\\'</think>\\')[0].rstrip(\\'\\\\n\\').split(\\'<think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n                {%- set content = content.split(\\'</think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\n<think>\\n\\' }}\\n{%- endif %}', 'qwen3.context_length': '40960', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Coder Final', 'general.type': 'model', 'qwen3.attention.layer_norm_rms_epsilon': '0.000001', 'general.size_label': '15B', 'qwen3.block_count': '40', 'qwen3.embedding_length': '5120', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen3.attention.head_count': '40', 'qwen3.feed_forward_length': '17408', 'qwen3.rope.freq_base': '1000000.000000', 'qwen3.attention.head_count_kv': '8', 'qwen3.attention.key_length': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- messages[0].content + '\\n\\n' }}\n",
            "    {%- endif %}\n",
            "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
            "{%- for message in messages[::-1] %}\n",
            "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
            "    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
            "        {%- set ns.multi_step_tool = false %}\n",
            "        {%- set ns.last_query_index = index %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- for message in messages %}\n",
            "    {%- if message.content is string %}\n",
            "        {%- set content = message.content %}\n",
            "    {%- else %}\n",
            "        {%- set content = '' %}\n",
            "    {%- endif %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {%- set reasoning_content = '' %}\n",
            "        {%- if message.reasoning_content is string %}\n",
            "            {%- set reasoning_content = message.reasoning_content %}\n",
            "        {%- else %}\n",
            "            {%- if '</think>' in content %}\n",
            "                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
            "                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n",
            "            {%- endif %}\n",
            "        {%- endif %}\n",
            "        {%- if loop.index0 > ns.last_query_index %}\n",
            "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
            "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
            "            {%- else %}\n",
            "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
            "            {%- endif %}\n",
            "        {%- else %}\n",
            "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
            "        {%- endif %}\n",
            "        {%- if message.tool_calls %}\n",
            "            {%- for tool_call in message.tool_calls %}\n",
            "                {%- if (loop.first and content) or (not loop.first) %}\n",
            "                    {{- '\\n' }}\n",
            "                {%- endif %}\n",
            "                {%- if tool_call.function %}\n",
            "                    {%- set tool_call = tool_call.function %}\n",
            "                {%- endif %}\n",
            "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
            "                {{- tool_call.name }}\n",
            "                {{- '\", \"arguments\": ' }}\n",
            "                {%- if tool_call.arguments is string %}\n",
            "                    {{- tool_call.arguments }}\n",
            "                {%- else %}\n",
            "                    {{- tool_call.arguments | tojson }}\n",
            "                {%- endif %}\n",
            "                {{- '}\\n</tool_call>' }}\n",
            "            {%- endfor %}\n",
            "        {%- endif %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\n",
            "<think>\n",
            "' }}\n",
            "{%- endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: ,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Generating with 24000 token limit...\n",
            "\n",
            "Okay, I need to solve this two-sum problem. Let's think about the examples given. The task is to find two numbers in the array that add up to the target and return their indices. The constraints say that each input has exactly one solution, and I can't use the same element twice. Oh right, and the follow-up asks for an algorithm better than O(n¬≤), so I need to think about efficient methods.\n",
            "\n",
            "The brute force approach would be to check every pair of elements, but that's O(n¬≤) time, which is probably too slow for large arrays. The user wants something better. So, what's a more efficient way?\n",
            "\n",
            "Hmm, using a hash map (dictionary in Python) could work. The idea is to store the values we've seen so far along with their indices. For each number in the array, we check if the complement (target - current number) exists in the hash map. If it does, we return the current index and the index from the hash map. If not, we add the current number to the hash map. This way, we only pass through the array once, making it O(n) time.\n",
            "\n",
            "Let me test this logic with the examples.\n",
            "\n",
            "Take example 1: nums = [2,7,11,15], target =9. Starting with an empty hash map. The first element is 2. The complement is 7. Not in the map, so add 2:0. Next element is 7. Complement is 2, which is in the map at index 0. So return [0,1]. That's correct.\n",
            "\n",
            "Example 2: nums = [3,2,4], target=6. First element 3, complement is 3. Not in map, add 3:0. Next 2, complement is 4. Not in map, add 2:1. Next 4, complement is 2, which is in the map at index 1. So return [1,2]. Correct.\n",
            "\n",
            "Example3: nums = [3,3], target=6. First 3, complement 3 not in map, add 3:0. Next 3, complement 3 is in map, so return 0 and 1. Correct.\n",
            "\n",
            "What about edge cases? Like when the two numbers are at the end of the array. Also, negative numbers? The algorithm should handle that since it's just checking the complement, regardless of their signs.\n",
            "\n",
            "Wait, what if the same number is used twice but the array has duplicates? Like in example 3, the two 3s. The hash map approach works because the first 3 is stored, and when the second 3 is processed, the complement is 3, which is present. So the indices are 0 and 1. That's correct.\n",
            "\n",
            "So the plan is to iterate through each number, check if the complement is in the hash map. If yes, return the indices. If not, add the current number and its index to the hash map.\n",
            "\n",
            "Now, code-wise in Python, I can initialize an empty dictionary. Then loop through the list with their indices. For each num in nums, calculate complement = target - num. Check if complement is in the dictionary. If it is, return [dict[complement], current index]. If not, add num to the dictionary with its index.\n",
            "\n",
            "Wait, but in Python, when you loop through the list, how to get the index? Using enumerate. So:\n",
            "\n",
            "for index, num in enumerate(nums):\n",
            "    complement = target - num\n",
            "    if complement in seen:\n",
            "        return [seen[complement], index]\n",
            "    seen[num] = index\n",
            "\n",
            "Yes. That should work.\n",
            "\n",
            "What about the case where the complement is the same as the current num but it's the second occurrence? Like example3. The first 3 is added to the dictionary. The second 3 checks the complement (3) which is present, so returns the indices 0 and 1. Correct.\n",
            "\n",
            "What if the complement is not in the array? According to the problem statement, there's exactly one solution, so this case won't happen. So the code can safely assume that once the complement is found, it's the correct pair.\n",
            "\n",
            "So the code should work. Let's code that.\n",
            "</think>\n",
            "\n",
            "```python\n",
            "def two_sum(nums, target):\n",
            "    seen = {}\n",
            "    for index, num in enumerate(nums):\n",
            "        complement = target - num\n",
            "        if complement in seen:\n",
            "            return [seen[complement], index]\n",
            "        seen[num] = index\n",
            "    return []\n",
            "```"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =     715.25 ms\n",
            "llama_perf_context_print: prompt eval time =     714.72 ms /   322 tokens (    2.22 ms per token,   450.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =   58473.03 ms /   942 runs   (   62.07 ms per token,    16.11 tokens per second)\n",
            "llama_perf_context_print:       total time =   61956.25 ms /  1264 tokens\n",
            "llama_perf_context_print:    graphs reused =        912\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " -------- Freeing memory --------\n",
            "Model connection closed.\n",
            "Python object deleted.\n",
            "C++ memory freed\n",
            "\n",
            " GPU Memory should now be freed.\n",
            "-------- Allocated GPU memory should be empty now --------\n"
          ]
        }
      ],
      "source": [
        "### IF YOU ENCOUNTER AN ERROR, do this! Go to Runtime > Restart session and run all\n",
        "\n",
        "REPO_ID = \"BigJuicyData/Anni-Q4_K_M-GGUF\"\n",
        "FILENAME = \"coder-final-q4_k_m.gguf\"\n",
        "CONTEXT_SIZE = 24000\n",
        "\n",
        "# --------------------------------------------\n",
        "# REPLACE THIS WITH YOUR PROGRAMMING QUESTION!\n",
        "# Áî®‰Ω†ÁöÑÁºñÁ®ãÈóÆÈ¢òÊõøÊç¢Ê≠§Â§ÑÔºÅÊúÄÂ•ΩÁî®Ëã±ÊñáÈ¢ò„ÄÇ\n",
        "# --------------------------------------------\n",
        "PROMPT=\"\"\"\n",
        "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
        "\n",
        "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
        "\n",
        "You can return the answer in any order.\n",
        "\n",
        "\n",
        "\n",
        "Example 1:\n",
        "\n",
        "Input: nums = [2,7,11,15], target = 9\n",
        "Output: [0,1]\n",
        "Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n",
        "\n",
        "Example 2:\n",
        "\n",
        "Input: nums = [3,2,4], target = 6\n",
        "Output: [1,2]\n",
        "\n",
        "Example 3:\n",
        "\n",
        "Input: nums = [3,3], target = 6\n",
        "Output: [0,1]\n",
        "\n",
        "\n",
        "\n",
        "Constraints:\n",
        "\n",
        "    2 <= nums.length <= 104\n",
        "    -109 <= nums[i] <= 109\n",
        "    -109 <= target <= 109\n",
        "    Only one valid answer exists.\n",
        "\n",
        "\n",
        "Follow-up: Can you come up with an algorithm that is less than O(n2) time complexity?\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the model\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=REPO_ID,\n",
        "    filename=FILENAME,\n",
        "    n_ctx=CONTEXT_SIZE,\n",
        "    n_gpu_layers=-1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Define the messages (Chat Format)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are Qwen. You are an expert algorithmic assistant.\n",
        "1. CORE OBJECTIVE: Solve the user's problem using the most efficient algorithm (optimal Time/Space Complexity).\n",
        "2. THOUGHT PROCESS: Use your <think> tag to analyze edge cases and plan the logic.\n",
        "3. OUTPUT: After reasoning, provide the Python solution.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": PROMPT,\n",
        "    }\n",
        "]\n",
        "\n",
        "# Run Inference\n",
        "print(f\"\\n Generating with {CONTEXT_SIZE} token limit...\\n\")\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages=messages,\n",
        "    max_tokens=CONTEXT_SIZE,\n",
        "\t\ttemperature=0.6,\n",
        "    top_p=0.95,\n",
        "    top_k=20,\n",
        "    min_p=0.0,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Print the streamed response\n",
        "for chunk in output:\n",
        "    delta = chunk['choices'][0]['delta']\n",
        "    if 'content' in delta:\n",
        "        print(delta['content'], end=\"\", flush=True)\n",
        "\n",
        "# Info\n",
        "print(\"\\nÂèØ‰ª•Áõ¥Êé•Â§çÂà∂‰ª•‰∏äÁöÑ‰ª£Á†Å„ÄÇ‰ª£Á†Å‰∏∫ ```python ``` ÂåÖÂÆπ„ÄÇ\")\n",
        "\n",
        "# Free the memory\n",
        "print(\"\\n-------- Freeing memory --------\")\n",
        "free_memory(llm)\n",
        "print(\"-------- Allocated GPU memory should be empty now --------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcAZGOqUMctu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
